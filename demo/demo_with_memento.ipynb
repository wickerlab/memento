{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration -- A machine learning pipeline with `MEMENTO`\n",
    "\n",
    "The benefit of using `MEMENTO`:\n",
    "\n",
    "- Avoiding all the copy and paste when running repeated experiments;\n",
    "- Experiments are running in parallel.\n",
    "- Focusing on the workflow of one experiment;\n",
    "- Keep all configurations in one place;\n",
    "- Automatic hashing each task;\n",
    "- Using checkpoints to keep tracking progress;\n",
    "- Send notification when the experiments fail or finish;\n",
    "\n",
    "## Install\n",
    "\n",
    "- `MEMENTO` is officially available on PyPl.\n",
    "- `MEMENTO` is portable and versatile. It doesn't tie to any machine learning packages, e.g., `sklearn` and `xgboost`.\n",
    "\n",
    "```bash\n",
    "# Using Python 3.9.x (Memento support Python 3.7, 3.8 and 3.9)\n",
    "conda create -n memento python=3.9\n",
    "conda activate memento\n",
    "\n",
    "# Install dependencies\n",
    "pip install memento-ml scikit-learn jupyterlab\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from memento import Config, ConsoleNotificationProvider, Context, Memento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_values(X, missing_rate=0.1):\n",
    "    \"\"\"Add missing features to n percent of samples. Remove 1 feature per sample.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    n_missing_samples = int(n_samples * missing_rate)\n",
    "\n",
    "    idx_missing_samples = np.random.choice(\n",
    "        n_samples, size=n_missing_samples, replace=True\n",
    "    )\n",
    "    idx_missing_features = np.random.randint(0, n_features, n_missing_samples)\n",
    "\n",
    "    X_missing = X.copy()\n",
    "    X_missing[idx_missing_samples, idx_missing_features] = np.nan\n",
    "    return X_missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dummy preprocessing class which returns X unchanged.\n",
    "DummyPreprocessor = FunctionTransformer(lambda x: x)\n",
    "\n",
    "# Using `partial` to avoid passing parameter in the experiment function.\n",
    "load_digits = functools.partial(datasets.load_digits, return_X_y=True)\n",
    "load_wine = functools.partial(datasets.load_wine, return_X_y=True)\n",
    "\n",
    "\n",
    "def load_breast_cancer():\n",
    "    \"\"\"Add missing values to Breast Cancer dataset.\"\"\"\n",
    "    X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "    X_missing = add_missing_values(X, missing_rate=0.1)\n",
    "    return X_missing, y\n",
    "\n",
    "\n",
    "Imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "\n",
    "# Put all parameters in a configuration matrix\n",
    "matrix = {\n",
    "    \"parameters\": {\n",
    "        \"dataset\": [\n",
    "            load_digits,\n",
    "            load_wine,\n",
    "            load_breast_cancer,\n",
    "        ],\n",
    "        \"preprocessing1\": [\n",
    "            DummyPreprocessor,\n",
    "            Imputer,\n",
    "        ],\n",
    "        \"preprocessing2\": [\n",
    "            DummyPreprocessor,\n",
    "            MinMaxScaler(),\n",
    "            StandardScaler(),\n",
    "        ],\n",
    "        \"classifier\": [\n",
    "            AdaBoostClassifier,\n",
    "            RandomForestClassifier,\n",
    "            SVC,\n",
    "        ],\n",
    "    },\n",
    "    \"settings\": {  # Set global values here\n",
    "        \"n_fold\": 5,\n",
    "    },\n",
    "    \"exclude\": [  # Only Breast Cancer dataset requires imputation.\n",
    "        {\"dataset\": load_breast_cancer, \"preprocessing1\": DummyPreprocessor},\n",
    "        {\"dataset\": load_digits, \"preprocessing1\": Imputer},\n",
    "        {\"dataset\": load_wine, \"preprocessing1\": Imputer},\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiment` function is the building block for **Memento**.\n",
    "It takes two parameters: `Context` and `Config`.\n",
    "Memento will automatically figure out how many tasks it need to create based on the configuration matrix, and execute them in parallel.\n",
    "Each task will execute this `experiment` function but with different parameters (inside `Config`).\n",
    "\n",
    "- The `Context` exposes a handler, so the user can access `checkpoint` in the `experiment` function.\n",
    "- The `Config` provides one set of parameter (from the configuration matrix) to the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(context: Context, config: Config):\n",
    "    \"\"\"This block contains the experiment with one set of parameters.\"\"\"\n",
    "    if context.checkpoint_exist():  # Based on the hashing value.\n",
    "        results = context.restore()  # Recover results from cache.\n",
    "    else:  # Cached results are not found. Running the experiment here:\n",
    "        # Access parameter:\n",
    "        X, y = config.dataset()\n",
    "        model = config.classifier()\n",
    "\n",
    "        # Access the global constant.\n",
    "        cv = config.settings[\"n_fold\"]\n",
    "\n",
    "        # Build and run the pipeline:\n",
    "        pipeline = make_pipeline(config.preprocessing1, config.preprocessing2, model)\n",
    "        results = cross_val_score(pipeline, X, y, cv=cv)\n",
    "\n",
    "        # Save results to the checkpoint:\n",
    "        # NOTE: The checkpoint can save any object. There is a list, not just a value.\n",
    "        context.checkpoint(results)\n",
    "    return results.mean() * 100  # Average from 5 runs in percentage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:memento.memento:Running configurations:\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:Exiting due to dry run\n"
     ]
    }
   ],
   "source": [
    "notification_provider = ConsoleNotificationProvider()\n",
    "\n",
    "# Do not actually run experiments, just log what would be run.\n",
    "results = Memento(experiment, notification_provider).run(matrix, dry_run=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:memento.memento:Running configurations:\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_digits at 0x00000225F197F4C0>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': functools.partial(<function load_wine at 0x00000225F197F310>, return_X_y=True), 'preprocessing1': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': FunctionTransformer(func=<function <lambda> at 0x00000225F339A040>), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': MinMaxScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._weight_boosting.AdaBoostClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.ensemble._forest.RandomForestClassifier'>}\n",
      "INFO:memento.memento:  {'dataset': <function load_breast_cancer at 0x00000225F339A310>, 'preprocessing1': SimpleImputer(), 'preprocessing2': StandardScaler(), 'classifier': <class 'sklearn.svm._classes.SVC'>}\n",
      "INFO:memento.memento:27/27 results retrieved from cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tasks completed\n"
     ]
    }
   ],
   "source": [
    "results = Memento(experiment, notification_provider).run(matrix)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we rerun the cell above, since there is no parameter changes and all results have been save the in the cache, the code will complete instantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.77\n",
      "93.71\n",
      "96.33\n",
      "26.77\n",
      "93.38\n",
      "95.99\n",
      "26.77\n",
      "93.6\n",
      "94.6\n",
      "80.84\n",
      "96.67\n",
      "66.35\n",
      "80.29\n",
      "97.22\n",
      "97.76\n",
      "80.29\n",
      "97.76\n",
      "98.33\n",
      "97.19\n",
      "96.31\n",
      "90.87\n",
      "97.54\n",
      "95.96\n",
      "97.72\n",
      "97.19\n",
      "95.96\n",
      "97.54\n"
     ]
    }
   ],
   "source": [
    "# Show avg. accuracy in percentage (Note that we multiple 100 in the experiment block)\n",
    "avg_accs = np.round([result.inner for result in results], 2)\n",
    "print(*avg_accs, sep=\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Runtime\n",
    "\n",
    "- The duration of the task is automatically recorded.\n",
    "- Saved as `datetime.timedelta`.\n",
    "- Runtime will be reserved for cached results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.993093\n",
      "1.316123\n",
      "0.487049\n",
      "0.980092\n",
      "1.447137\n",
      "0.370038\n",
      "1.315127\n",
      "1.386133\n",
      "0.50805\n",
      "0.390038\n",
      "0.814079\n",
      "0.037004\n",
      "0.430046\n",
      "0.780074\n",
      "0.031003\n",
      "0.385037\n",
      "0.718068\n",
      "0.083009\n",
      "0.706064\n",
      "0.881084\n",
      "0.118013\n",
      "0.770074\n",
      "1.232118\n",
      "0.054007\n",
      "0.750072\n",
      "1.213115\n",
      "0.084006\n"
     ]
    }
   ],
   "source": [
    "# Convert TimeDelta to Seconds\n",
    "time_deltas = [result.runtime.total_seconds() for result in results]\n",
    "print(*time_deltas, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8a14c2cb3206237cd982c8447566eef72ed9b8a95eca3109a9df28d87592576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
